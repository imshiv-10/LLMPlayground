{"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96}],"instance_type":"ml.g5.4xlarge","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tested on a GPU P100","metadata":{}},{"cell_type":"code","source":"%pip install -Uq torch==2.0.1 \\\n  transformers==4.33.0 \\\n  sentencepiece==0.1.99 \\\n  accelerate==0.22.0 # needed for low_cpu_mem_usage parameter","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-29T03:01:25.225606Z","iopub.execute_input":"2024-04-29T03:01:25.226292Z","iopub.status.idle":"2024-04-29T03:03:36.321012Z","shell.execute_reply.started":"2024-04-29T03:01:25.226258Z","shell.execute_reply":"2024-04-29T03:03:36.319787Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.33.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install sentencepiece -U --quiet","metadata":{"execution":{"iopub.status.busy":"2024-04-29T03:03:36.322958Z","iopub.execute_input":"2024-04-29T03:03:36.323274Z","iopub.status.idle":"2024-04-29T03:03:49.758825Z","shell.execute_reply.started":"2024-04-29T03:03:36.323246Z","shell.execute_reply":"2024-04-29T03:03:49.757778Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-04-29T03:03:49.760757Z","iopub.execute_input":"2024-04-29T03:03:49.761154Z","iopub.status.idle":"2024-04-29T03:03:51.330392Z","shell.execute_reply.started":"2024-04-29T03:03:49.761118Z","shell.execute_reply":"2024-04-29T03:03:51.329537Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T03:03:51.332228Z","iopub.execute_input":"2024-04-29T03:03:51.332560Z","iopub.status.idle":"2024-04-29T03:03:51.362348Z","shell.execute_reply.started":"2024-04-29T03:03:51.332537Z","shell.execute_reply":"2024-04-29T03:03:51.361117Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import LlamaTokenizer\n\nmodel_checkpoint = \"NousResearch/Llama-2-7b-hf\"\ntokenizer = LlamaTokenizer.from_pretrained(model_checkpoint)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-29T03:03:51.363795Z","iopub.execute_input":"2024-04-29T03:03:51.364127Z","iopub.status.idle":"2024-04-29T03:03:53.907038Z","shell.execute_reply.started":"2024-04-29T03:03:51.364079Z","shell.execute_reply":"2024-04-29T03:03:53.906035Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10c8cfcb20c24039840bdd8c0e92d6e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3990fa816bca4e0598076a98678808fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdf4478c7ab349d3b7e151e38dad0f26"}},"metadata":{}}]},{"cell_type":"code","source":"# based on https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/blob/1c9e29b70b1aaa7133d3c7d7b59a92d840e92e6d/llama_cpp_chat_completion_wrapper.py\n\nfrom typing import List\nfrom typing import Literal\n\nfrom typing import TypedDict\n\nfrom transformers import PreTrainedTokenizer\n\nRole = Literal[\"system\", \"user\", \"assistant\"]\n\nclass Message(TypedDict):\n    role: Role\n    content: str\n\nMessageList = List[Message]\n\nBEGIN_INST, END_INST = \"[INST] \", \" [/INST] \"\nBEGIN_SYS, END_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n\ndef convert_list_of_message_lists_to_input_prompt(list_of_message_lists: List[MessageList], tokenizer: PreTrainedTokenizer) -> List[str]:\n    input_prompts: List[str] = []\n    print(type(list_of_message_lists))\n    print(type(list_of_message_lists[0]))    \n    for message_list in list_of_message_lists:\n        if message_list[0][\"role\"] == \"system\":\n            content = \"\".join([BEGIN_SYS, message_list[0][\"content\"], END_SYS, message_list[1][\"content\"]])\n            message_list = [{\"role\": message_list[1][\"role\"], \"content\": content}] + message_list[2:]\n\n        if not (\n            all([msg[\"role\"] == \"user\" for msg in message_list[::2]])\n            and all([msg[\"role\"] == \"assistant\" for msg in message_list[1::2]])\n        ):\n            raise ValueError(\n                \"Format must be in this order: 'system', 'user', 'assistant' roles.\\nAfter that, you can alternate between user and assistant multiple times\"\n            )\n\n        eos = tokenizer.eos_token\n        bos = tokenizer.bos_token\n        input_prompt = \"\".join(\n            [\n                \"\".join([bos, BEGIN_INST, (prompt[\"content\"]).strip(), END_INST, (answer[\"content\"]).strip(), eos])\n                for prompt, answer in zip(message_list[::2], message_list[1::2])\n            ]\n        )\n\n        if message_list[-1][\"role\"] != \"user\":\n            raise ValueError(f\"Last message must be from user role. Instead, you sent from {message_list[-1]['role']} role\")\n\n        input_prompt += \"\".join([bos, BEGIN_INST, (message_list[-1][\"content\"]).strip(), END_INST])\n\n        input_prompts.append(input_prompt)\n\n    return input_prompts","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-29T03:03:58.859071Z","iopub.execute_input":"2024-04-29T03:03:58.859613Z","iopub.status.idle":"2024-04-29T03:03:58.873217Z","shell.execute_reply.started":"2024-04-29T03:03:58.859582Z","shell.execute_reply":"2024-04-29T03:03:58.872317Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"system_message = Message()\nsystem_message[\"role\"] = \"system\"\nsystem_message[\"content\"] = \"Answer only with emojis\"\nprint(system_message)\n\nuser_message = Message()\nuser_message[\"role\"] = \"user\"\nuser_message[\"content\"] = \"Who won the 2016 baseball World Series?\"\nprint(user_message)\n\n# assistant_message = Message()\n# assistant_message.role = \"assistant\"\n# assistant_message.content = \"\"\n\nlist_of_messages = list()\nlist_of_messages.append(system_message)\nlist_of_messages.append(user_message)\n\nlist_of_message_lists = list()\nlist_of_message_lists.append(list_of_messages)\n\nprompt = convert_list_of_message_lists_to_input_prompt(list_of_message_lists, tokenizer)\nprint(prompt)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-29T03:04:00.118805Z","iopub.execute_input":"2024-04-29T03:04:00.119190Z","iopub.status.idle":"2024-04-29T03:04:00.126641Z","shell.execute_reply.started":"2024-04-29T03:04:00.119159Z","shell.execute_reply":"2024-04-29T03:04:00.125754Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'role': 'system', 'content': 'Answer only with emojis'}\n{'role': 'user', 'content': 'Who won the 2016 baseball World Series?'}\n<class 'list'>\n<class 'list'>\n['<s>[INST] <<SYS>>\\nAnswer only with emojis\\n<</SYS>>\\n\\nWho won the 2016 baseball World Series? [/INST] ']\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import LlamaForCausalLM\n\nmodel = LlamaForCausalLM.from_pretrained(\n    model_checkpoint,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n).to(device)\n\n# model = model.eval()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T03:04:01.197384Z","iopub.execute_input":"2024-04-29T03:04:01.197718Z","iopub.status.idle":"2024-04-29T03:09:05.960341Z","shell.execute_reply.started":"2024-04-29T03:04:01.197692Z","shell.execute_reply":"2024-04-29T03:09:05.958199Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-04-29 03:04:03.586814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-29 03:04:03.586930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-29 03:04:03.733236: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72463118d873475795f90b4c9a714316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d62950013c45719e3e02ed12da1d56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb05cfb425e4b3a8a32ba2c5de3ef1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a4980a353204e05891253d4a28370c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abbe4bda1a8c473fb6dd6162fdb413e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f8be89dbb9e4cdda3c7a7a17fe3fa07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc51351d8974c4b8aa6fa336313cf8f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\ntokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n\ntokenized_prompt_input_ids = tokenized_prompt.input_ids.to(device)\n\nprint(f'prompt is {len(tokenized_prompt[\"input_ids\"][0])} tokens')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-29T03:09:05.962899Z","iopub.execute_input":"2024-04-29T03:09:05.963264Z","iopub.status.idle":"2024-04-29T03:09:08.693966Z","shell.execute_reply.started":"2024-04-29T03:09:05.963220Z","shell.execute_reply":"2024-04-29T03:09:08.692992Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"prompt is 41 tokens\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_prompt_input_ids","metadata":{"execution":{"iopub.status.busy":"2024-04-29T03:09:08.695209Z","iopub.execute_input":"2024-04-29T03:09:08.695540Z","iopub.status.idle":"2024-04-29T03:09:08.734596Z","shell.execute_reply.started":"2024-04-29T03:09:08.695514Z","shell.execute_reply":"2024-04-29T03:09:08.733698Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[    1,     1, 29961, 25580, 29962,  3532, 14816, 29903,  6778,    13,\n         22550,   871,   411,   953,  3848,   275,    13, 29966,   829, 14816,\n         29903,  6778,    13,    13, 22110,  2113,   278, 29871, 29906, 29900,\n         29896, 29953, 21573,  2787, 10488, 29973,   518, 29914, 25580, 29962,\n         29871]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GenerationConfig\n\ngeneration_config = GenerationConfig(max_new_tokens=2000)\n\npipeline = pipeline(\"text-generation\", \n                    model=model,\n                    tokenizer= tokenizer, \n                    device = device,\n                    generation_config=generation_config)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-29T03:09:08.736905Z","iopub.execute_input":"2024-04-29T03:09:08.737566Z","iopub.status.idle":"2024-04-29T03:09:08.748131Z","shell.execute_reply.started":"2024-04-29T03:09:08.737503Z","shell.execute_reply":"2024-04-29T03:09:08.747133Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"output = pipeline(prompt)[0]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-04-29T03:09:08.749179Z","iopub.execute_input":"2024-04-29T03:09:08.749513Z","iopub.status.idle":"2024-04-29T03:10:58.433638Z","shell.execute_reply.started":"2024-04-29T03:09:08.749468Z","shell.execute_reply":"2024-04-29T03:10:58.432611Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"len(output)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T03:14:00.952578Z","iopub.execute_input":"2024-04-29T03:14:00.953311Z","iopub.status.idle":"2024-04-29T03:14:00.959002Z","shell.execute_reply.started":"2024-04-29T03:14:00.953281Z","shell.execute_reply":"2024-04-29T03:14:00.958091Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"print(output[0][\"generated_text\"][0:200])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T03:15:08.051523Z","iopub.execute_input":"2024-04-29T03:15:08.052397Z","iopub.status.idle":"2024-04-29T03:15:08.057236Z","shell.execute_reply.started":"2024-04-29T03:15:08.052359Z","shell.execute_reply":"2024-04-29T03:15:08.056261Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"<s>[INST] <<SYS>>\nAnswer only with emojis\n<</SYS>>\n\nWho won the 2016 baseball World Series? [/INST] \n\n[INST] <<SYS>>\nAnswer only with emojis\n<</SYS>>\n\nWho won the 2016 baseball World Series? [/INST] \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}